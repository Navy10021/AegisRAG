import os
import json
import logging
from dataclasses import dataclass, field
from typing import List, Optional, Dict, Tuple
from datetime import datetime
from functools import lru_cache
import asyncio

from pydantic import BaseModel, Field, validator
import openai

# embedding_utilsì˜ ìƒˆë¡œìš´ ê¸°ëŠ¥ë“¤ ì„í¬íŠ¸
from .embedding_utils import (
    get_embedding_model,
    encode_texts,
    search_top_policies,
    hybrid_search,
    cosine_similarity,
    clear_model_cache,
    get_embedding_dimension,
    validate_embeddings,
    SENTENCE_TRANSFORMERS_AVAILABLE
)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ==================== ë°ì´í„° ëª¨ë¸ ====================

@dataclass
class SecurityPolicy:
    """ë³´ì•ˆ ì •ì±… (ë¶ˆë³€ì„± ì¶”ê°€)"""
    id: str
    title: str
    content: str
    severity: str  # critical, high, medium, low
    keywords: List[str] = field(default_factory=list)
    
    def __post_init__(self):
        """ìœ íš¨ì„± ê²€ì¦"""
        valid_severities = ['critical', 'high', 'medium', 'low']
        if self.severity not in valid_severities:
            raise ValueError(f"severityëŠ” {valid_severities} ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤")
        if not self.keywords:
            logger.warning(f"ì •ì±… {self.id}ì— í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤")
    
    def __hash__(self):
        """í•´ì‹± ê°€ëŠ¥í•˜ë„ë¡ (ìºì‹±ìš©)"""
        return hash(self.id)


class AnalysisResult(BaseModel):
    """ë¶„ì„ ê²°ê³¼ (ê²€ì¦ ê°•í™”)"""
    text: str
    risk_score: float = Field(ge=0, le=100)
    risk_level: str
    violations: List[str] = Field(default_factory=list)
    threats: List[str] = Field(default_factory=list)
    explanation: str
    related_policies: List[str] = Field(default_factory=list)
    policy_similarities: Dict[str, float] = Field(default_factory=dict)  # ì‹ ê·œ ì¶”ê°€
    processing_time: float = 0.0
    timestamp: str = Field(default_factory=lambda: datetime.now().isoformat())
    
    @validator('risk_level')
    def validate_risk_level(cls, v):
        """ìœ„í—˜ë„ ë ˆë²¨ ê²€ì¦"""
        valid_levels = ['CRITICAL', 'HIGH', 'MEDIUM', 'LOW']
        if v not in valid_levels:
            raise ValueError(f"risk_levelì€ {valid_levels} ì¤‘ í•˜ë‚˜ì—¬ì•¼ í•©ë‹ˆë‹¤")
        return v
    
    class Config:
        json_encoders = {
            datetime: lambda v: v.isoformat()
        }


# ==================== ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ ====================

def load_policies(json_path: str) -> List[SecurityPolicy]:
    """ì •ì±… ë¡œë“œ (ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”)"""
    if not os.path.exists(json_path):
        raise FileNotFoundError(f"ë³´ì•ˆ ì •ì±… JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")
    
    try:
        with open(json_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        if not isinstance(data, list):
            raise ValueError("ì •ì±… JSONì€ ë¦¬ìŠ¤íŠ¸ í˜•íƒœì—¬ì•¼ í•©ë‹ˆë‹¤")
        
        policies = [SecurityPolicy(**p) for p in data]
        logger.info(f"âœ… {len(policies)}ê°œ ì •ì±… ë¡œë“œ ì™„ë£Œ: {json_path}")
        return policies
        
    except json.JSONDecodeError as e:
        raise ValueError(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
    except Exception as e:
        raise Exception(f"ì •ì±… ë¡œë“œ ì‹¤íŒ¨: {e}")


# ==================== ë©”ì¸ ë¶„ì„ê¸° ====================

class RAGSecurityAnalyzer:
    """RAG ê¸°ë°˜ ë³´ì•ˆ ë¶„ì„ê¸° (ìµœì¢… ìµœì í™” ë²„ì „)"""
    
    # í´ë˜ìŠ¤ ë ˆë²¨ ìƒìˆ˜
    THREAT_PATTERNS = {
        'í‡´ì‚¬': ('ë‚´ë¶€ì ìœ„í˜‘ - í‡´ì‚¬', 30),
        'ì´ì§': ('ë‚´ë¶€ì ìœ„í˜‘ - ì´ì§', 25),
        'ê²½ìŸì‚¬': ('ì •ë³´ ìœ ì¶œ ìœ„í—˜', 35),
        'ë‹¤ìš´ë¡œë“œ': ('ë°ì´í„° ë°˜ì¶œ', 20),
        'USB': ('ì™¸ë¶€ ë°˜ì¶œ', 20),
        'í´ë¼ìš°ë“œ': ('ì™¸ë¶€ í´ë¼ìš°ë“œ ê³µìœ ', 30),
        'ê°œì¸ì •ë³´': ('ê°œì¸ì •ë³´ ì¹¨í•´', 25),
        'ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸': ('ê°œì¸ì •ë³´ ì¹¨í•´', 25),
        'í•µì‹¬ê¸°ìˆ ': ('ê¸°ìˆ  ìœ ì¶œ', 40),
        'ì„¤ê³„ë„': ('ê¸°ë°€ ì„¤ê³„ë„ ìœ ì¶œ', 35),
        'R&D': ('ì—°êµ¬ê°œë°œ ìë£Œ ìœ ì¶œ', 30),
        'ê³ ê°ë¦¬ìŠ¤íŠ¸': ('ì˜ì—…ë¹„ë°€ ì¹¨í•´', 25),
        'ê°€ê²©ì •ì±…': ('ì˜ì—…ë¹„ë°€ ì¹¨í•´', 20),
        'ë§ˆì¼€íŒ…ì „ëµ': ('ì˜ì—… ì „ëµ ìœ ì¶œ', 20),
        'íŒë§¤ì „ëµ': ('ì˜ì—… ì „ëµ ìœ ì¶œ', 20),
        'ë¹„ì¸ê°€': ('ë¹„ì¸ê°€ ì ‘ê·¼', 15),
        'ê¶Œí•œ': ('ê¶Œí•œ ë‚¨ìš©', 15),
        'ë¡œê·¸ì¸': ('ì‹œìŠ¤í…œ ì ‘ê·¼ ìœ„í—˜', 10),
        'ì´ë©”ì¼': ('ì™¸ë¶€ ì „ì†¡ ìœ„í—˜', 15),
        'ë©”ì‹ ì €': ('ì™¸ë¶€ ì „ì†¡ ìœ„í—˜', 15),
    }
    
    SEVERITY_MULTIPLIER = {
        "critical": 1.5,
        "high": 1.2,
        "medium": 1.0,
        "low": 0.8
    }
    
    SEVERITY_POINTS = {
        'critical': 30,
        'high': 20,
        'medium': 10,
        'low': 5
    }
    
    def __init__(
        self,
        policies: List[SecurityPolicy],
        api_key: Optional[str] = None,
        use_llm: bool = True,
        use_embeddings: bool = True,
        search_mode: str = "hybrid",  # "embedding", "keyword", "hybrid"
        verbose: bool = True,
        cache_size: int = 128
    ):
        """
        ì´ˆê¸°í™”
        
        Args:
            policies: ë³´ì•ˆ ì •ì±… ë¦¬ìŠ¤íŠ¸
            api_key: OpenAI API í‚¤
            use_llm: LLM ì‚¬ìš© ì—¬ë¶€
            use_embeddings: ì„ë² ë”© ì‚¬ìš© ì—¬ë¶€
            search_mode: ê²€ìƒ‰ ëª¨ë“œ ("embedding"|"keyword"|"hybrid")
            verbose: ìƒì„¸ ë¡œê·¸ ì¶œë ¥
            cache_size: LRU ìºì‹œ í¬ê¸°
        """
        if not policies:
            raise ValueError("ìµœì†Œ 1ê°œ ì´ìƒì˜ ì •ì±…ì´ í•„ìš”í•©ë‹ˆë‹¤")
        
        self.policies = policies
        self.use_llm = use_llm
        self.use_embeddings = use_embeddings and SENTENCE_TRANSFORMERS_AVAILABLE
        self.search_mode = search_mode
        self.verbose = verbose
        self.cache_size = cache_size
        
        # í†µê³„ ì¶”ì 
        self.stats = {
            'total_analyzed': 0,
            'llm_calls': 0,
            'rule_based_calls': 0,
            'cache_hits': 0,
            'errors': 0,
            'avg_policy_similarity': 0.0  # ì‹ ê·œ ì¶”ê°€
        }
        
        # API í‚¤ ì„¤ì •
        if api_key:
            os.environ['OPENAI_API_KEY'] = api_key
        self.api_key = os.getenv('OPENAI_API_KEY')
        
        # ì„ë² ë”© ëª¨ë¸ ì¤€ë¹„
        if self.use_embeddings:
            try:
                logger.info("ğŸ”§ ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
                self.embedding_model = get_embedding_model()
                
                if self.embedding_model is None:
                    raise Exception("ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
                
                # ì •ì±… ì„ë² ë”© ìƒì„±
                policy_texts = [f"{p.title}. {p.content}" for p in self.policies]
                self.policy_embeddings = encode_texts(
                    policy_texts,
                    model=self.embedding_model,
                    batch_size=32,
                    show_progress=False,
                    normalize=True
                )
                
                # ì„ë² ë”© ê²€ì¦
                expected_dim = get_embedding_dimension(self.embedding_model)
                if not validate_embeddings(self.policy_embeddings, expected_dim):
                    raise Exception("ì„ë² ë”© ê²€ì¦ ì‹¤íŒ¨")
                
                logger.info(f"âœ… ì„ë² ë”© ì´ˆê¸°í™” ì™„ë£Œ (ì°¨ì›: {expected_dim})")
                
            except Exception as e:
                logger.warning(f"âš ï¸ ì„ë² ë”© ì´ˆê¸°í™” ì‹¤íŒ¨: {e}, í‚¤ì›Œë“œ ê²€ìƒ‰ ì‚¬ìš©")
                self.use_embeddings = False
                self.search_mode = "keyword"
        
        # LLM ì´ˆê¸°í™”
        if self.use_llm:
            if self.api_key:
                openai.api_key = self.api_key
                logger.info("âœ… OpenAI LLM ëª¨ë“œ í™œì„±í™”")
            else:
                logger.warning("âš ï¸ API í‚¤ ì—†ìŒ, ê·œì¹™ ê¸°ë°˜ ëª¨ë“œ ì‚¬ìš©")
                self.use_llm = False
        
        # ê²€ìƒ‰ ëª¨ë“œ ê²€ì¦
        if self.search_mode not in ["embedding", "keyword", "hybrid"]:
            logger.warning(f"ì˜ëª»ëœ ê²€ìƒ‰ ëª¨ë“œ: {self.search_mode}, 'hybrid'ë¡œ ë³€ê²½")
            self.search_mode = "hybrid"
        
        if self.search_mode in ["embedding", "hybrid"] and not self.use_embeddings:
            logger.warning("ì„ë² ë”© ë¹„í™œì„±í™”, ê²€ìƒ‰ ëª¨ë“œë¥¼ 'keyword'ë¡œ ë³€ê²½")
            self.search_mode = "keyword"
        
        # ìºì‹œ ì´ˆê¸°í™”
        self._search_policies_cached = lru_cache(maxsize=cache_size)(
            self._search_policies_impl
        )
        
        logger.info(f"âœ… ë¶„ì„ê¸° ì¤€ë¹„ ì™„ë£Œ! (ê²€ìƒ‰ ëª¨ë“œ: {self.search_mode})\n")
    
    def _search_policies_impl(self, text: str) -> Tuple:
        """ì •ì±… ê²€ìƒ‰ (ìºì‹±ìš© ë‚´ë¶€ ë©”ì„œë“œ)"""
        if self.search_mode == "hybrid":
            # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ìµœê³  ì„±ëŠ¥)
            results = hybrid_search(
                text,
                self.policies,
                self.policy_embeddings,
                top_k=3,
                embedding_weight=0.7,
                keyword_weight=0.3,
                model=self.embedding_model,
                return_scores=True
            )
            return tuple(results)
            
        elif self.search_mode == "embedding":
            # ìˆœìˆ˜ ì„ë² ë”© ê²€ìƒ‰
            results = search_top_policies(
                text,
                self.policies,
                self.policy_embeddings,
                top_k=3,
                min_similarity=0.0,
                model=self.embedding_model,
                return_scores=True
            )
            return tuple(results)
            
        else:  # keyword
            # í‚¤ì›Œë“œ ê²€ìƒ‰
            from .embedding_utils import keyword_based_search
            results = keyword_based_search(
                text,
                self.policies,
                top_k=3,
                return_scores=True
            )
            return tuple(results)
    
    def _search_policies(self, text: str) -> List[Tuple[SecurityPolicy, float]]:
        """ì •ì±… ê²€ìƒ‰ (ìºì‹œ í™œìš©, ì ìˆ˜ í¬í•¨)"""
        cache_info_before = self._search_policies_cached.cache_info()
        result = self._search_policies_cached(text)
        cache_info_after = self._search_policies_cached.cache_info()
        
        # ìºì‹œ íˆíŠ¸ í™•ì¸
        if cache_info_after.hits > cache_info_before.hits:
            self.stats['cache_hits'] += 1
            if self.verbose:
                logger.debug("ğŸ’¾ ìºì‹œ íˆíŠ¸")
        
        return list(result)
    
    def analyze(self, text: str) -> AnalysisResult:
        """
        í…ìŠ¤íŠ¸ ë¶„ì„ (ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì¶”ê°€)
        
        Args:
            text: ë¶„ì„í•  í…ìŠ¤íŠ¸
            
        Returns:
            AnalysisResult: ë¶„ì„ ê²°ê³¼
        """
        start_time = datetime.now()
        
        try:
            # ì…ë ¥ ê²€ì¦
            if not text or not text.strip():
                raise ValueError("ë¶„ì„í•  í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
            
            # ì •ì±… ê²€ìƒ‰ (ì ìˆ˜ í¬í•¨)
            policy_results = self._search_policies(text)
            policies = [policy for policy, _ in policy_results]
            similarities = {policy.id: score for policy, score in policy_results}
            
            # í‰ê·  ìœ ì‚¬ë„ ì¶”ì 
            if similarities:
                avg_sim = sum(similarities.values()) / len(similarities)
                self.stats['avg_policy_similarity'] = (
                    self.stats['avg_policy_similarity'] * self.stats['total_analyzed'] + avg_sim
                ) / (self.stats['total_analyzed'] + 1)
            
            if self.verbose:
                logger.info(f"ğŸ“š ê´€ë ¨ ì •ì±… {len(policies)}ê°œ:")
                for policy, score in policy_results:
                    logger.info(f"   {score:.3f} - [{policy.id}] {policy.title}")
            
            # ë¶„ì„ ìˆ˜í–‰
            if self.use_llm:
                result = self._analyze_with_llm(text, policies)
                self.stats['llm_calls'] += 1
            else:
                result = self._analyze_with_rules(text, policies, similarities)
                self.stats['rule_based_calls'] += 1
            
            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            result.related_policies = [p.id for p in policies]
            result.policy_similarities = similarities
            result.processing_time = (datetime.now() - start_time).total_seconds()
            
            self.stats['total_analyzed'] += 1
            
            return result
            
        except Exception as e:
            self.stats['errors'] += 1
            logger.error(f"âŒ ë¶„ì„ ì˜¤ë¥˜: {e}", exc_info=True)
            
            # ì•ˆì „í•œ ê¸°ë³¸ ê²°ê³¼ ë°˜í™˜
            return AnalysisResult(
                text=text,
                risk_score=0.0,
                risk_level='LOW',
                explanation=f"ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}",
                processing_time=(datetime.now() - start_time).total_seconds()
            )
    
    def _analyze_with_llm(
        self,
        text: str,
        policies: List[SecurityPolicy]
    ) -> AnalysisResult:
        """LLM ê¸°ë°˜ ë¶„ì„ (ì¬ì‹œë„ ë¡œì§ ì¶”ê°€)"""
        policy_context = "\n".join([
            f"[{p.id}] {p.title} ({p.severity}): {p.content}"
            for p in policies
        ])
        
        prompt = f"""ë‹¹ì‹ ì€ ë³´ì•ˆ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì„¸ìš”.

ê´€ë ¨ ë³´ì•ˆ ì •ì±…:
{policy_context}

ë¶„ì„í•  í…ìŠ¤íŠ¸:
"{text}"

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "risk_score": <0-100>,
    "risk_level": "<CRITICAL|HIGH|MEDIUM|LOW>",
    "violations": ["ìœ„ë°˜ëœ ì •ì±… IDë“¤"],
    "threats": ["íƒì§€ëœ ìœ„í˜‘ë“¤"],
    "explanation": "ë¶„ì„ ì„¤ëª…"
}}"""

        max_retries = 3
        for attempt in range(max_retries):
            try:
                if self.verbose:
                    logger.info(f"ğŸ“ LLM í˜¸ì¶œ (ì‹œë„ {attempt + 1}/{max_retries})")
                
                response = openai.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[
                        {
                            "role": "system",
                            "content": "ë³´ì•ˆ ë¶„ì„ ì „ë¬¸ê°€. ìš”ì²­ëœ JSON ìŠ¤í‚¤ë§ˆë§Œ ì‘ë‹µ."
                        },
                        {"role": "user", "content": prompt}
                    ],
                    temperature=0.1,
                    response_format={"type": "json_object"},
                    timeout=30
                )
                
                result_dict = json.loads(response.choices[0].message.content)
                
                # ê²°ê³¼ ê²€ì¦
                if not all(k in result_dict for k in ['risk_score', 'risk_level']):
                    raise ValueError("ì‘ë‹µì— í•„ìˆ˜ í•„ë“œê°€ ì—†ìŠµë‹ˆë‹¤")
                
                return AnalysisResult(text=text, **result_dict)
                
            except openai.APIError as e:
                logger.warning(f"âš ï¸ OpenAI API ì˜¤ë¥˜ (ì‹œë„ {attempt + 1}): {e}")
                if attempt == max_retries - 1:
                    logger.error("âŒ LLM í˜¸ì¶œ ì‹¤íŒ¨, ê·œì¹™ ê¸°ë°˜ìœ¼ë¡œ ì „í™˜")
                    return self._analyze_with_rules(text, policies, {})
                continue
                
            except Exception as e:
                logger.error(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
                return self._analyze_with_rules(text, policies, {})
    
    def _analyze_with_rules(
        self,
        text: str,
        policies: List[SecurityPolicy],
        similarities: Dict[str, float]
    ) -> AnalysisResult:
        """ê·œì¹™ ê¸°ë°˜ ë¶„ì„ (ìœ ì‚¬ë„ ì ìˆ˜ ë°˜ì˜)"""
        violations = []
        threats = []
        score = 0.0
        
        # ìœ„í˜‘ íƒì§€
        detected_keywords = []
        for keyword, (threat, points) in self.THREAT_PATTERNS.items():
            if keyword in text:
                threats.append(threat)
                score += points
                detected_keywords.append(keyword)
        
        # ì •ì±… ìœ„ë°˜ í™•ì¸ (ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜ ì ìš©)
        for policy in policies:
            matched_keywords = [kw for kw in policy.keywords if kw in text]
            
            if matched_keywords:
                violations.append(policy.id)
                
                # ê¸°ë³¸ ì ìˆ˜
                base_points = self.SEVERITY_POINTS.get(policy.severity, 10)
                multiplier = self.SEVERITY_MULTIPLIER.get(policy.severity, 1.0)
                
                # í‚¤ì›Œë“œ ë§¤ì¹­ ë¹„ìœ¨
                match_ratio = len(matched_keywords) / max(len(policy.keywords), 1)
                
                # ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜ (ìˆìœ¼ë©´ ì ìš©)
                similarity_weight = similarities.get(policy.id, 0.5)
                
                # ìµœì¢… ì ìˆ˜ = ê¸°ë³¸ì ìˆ˜ Ã— ì‹¬ê°ë„ Ã— ë§¤ì¹­ë¹„ìœ¨ Ã— ìœ ì‚¬ë„
                policy_score =  base_points * multiplier * (0.8 + similarity_weight * 0.7) * (0.6 + match_ratio * 0.4)
                score += policy_score
                
                if self.verbose:
                    logger.debug(
                        f"ì •ì±… {policy.id}: "
                        f"base={base_points}, mult={multiplier:.1f}, "
                        f"match={match_ratio:.2f}, sim={similarity_weight:.2f} "
                        f"â†’ {policy_score:.1f}ì "
                    )
        
        # ì ìˆ˜ ì •ê·œí™” ë° ë ˆë²¨ ê²°ì •
        score = min(score, 100.0)
        
        if score >= 60:
            level = "CRITICAL"
        elif score >= 40:
            level = "HIGH"
        elif score >= 20:
            level = "MEDIUM"
        else:
            level = "LOW"
        
        # ì„¤ëª… ìƒì„±
        explanation_parts = []
        if threats:
            explanation_parts.append(f"{len(threats)}ê°œ ìœ„í˜‘ íƒì§€")
        if violations:
            explanation_parts.append(f"{len(violations)}ê°œ ì •ì±… ìœ„ë°˜")
        if detected_keywords:
            explanation_parts.append(f"ì£¼ìš” í‚¤ì›Œë“œ: {', '.join(detected_keywords[:5])}")
        if similarities:
            avg_sim = sum(similarities.values()) / len(similarities)
            explanation_parts.append(f"í‰ê·  ì •ì±… ìœ ì‚¬ë„: {avg_sim:.2f}")
        
        explanation = " | ".join(explanation_parts) if explanation_parts else "ìœ„í˜‘ ì—†ìŒ"
        
        return AnalysisResult(
            text=text,
            risk_score=round(score, 1),
            risk_level=level,
            violations=violations,
            threats=threats,
            explanation=explanation
        )
    
    async def analyze_async(self, text: str) -> AnalysisResult:
        """ë¹„ë™ê¸° ë¶„ì„"""
        return await asyncio.to_thread(self.analyze, text)
    
    async def analyze_batch_async(
        self,
        texts: List[str],
        max_concurrent: int = 5
    ) -> List[AnalysisResult]:
        """ë¹„ë™ê¸° ë°°ì¹˜ ë¶„ì„ (ë™ì‹œ ì‹¤í–‰ ìˆ˜ ì œì–´)"""
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def analyze_with_limit(text: str) -> AnalysisResult:
            async with semaphore:
                return await self.analyze_async(text)
        
        tasks = [analyze_with_limit(text) for text in texts]
        return await asyncio.gather(*tasks, return_exceptions=False)
    
    def analyze_batch(self, texts: List[str]) -> List[AnalysisResult]:
        """ë™ê¸° ë°°ì¹˜ ë¶„ì„"""
        return [self.analyze(t) for t in texts]
    
    def print_result(self, result: AnalysisResult):
        """ê²°ê³¼ ì¶œë ¥ (ìœ ì‚¬ë„ ì ìˆ˜ ì¶”ê°€)"""
        emoji = {
            "CRITICAL": "ğŸ”´",
            "HIGH": "ğŸŸ ",
            "MEDIUM": "ğŸŸ¡",
            "LOW": "ğŸŸ¢"
        }[result.risk_level]
        
        print("=" * 80)
        print(f"ğŸ›¡ï¸  ë³´ì•ˆ ë¶„ì„ ê²°ê³¼")
        print("=" * 80)
        print(f"\nğŸ“ í…ìŠ¤íŠ¸: {result.text}")
        print(f"ğŸ•’ ë¶„ì„ ì‹œê°„: {result.processing_time:.3f}ì´ˆ")
        print(f"ğŸ“… íƒ€ì„ìŠ¤íƒ¬í”„: {result.timestamp}\n")
        print(f"{emoji} ìœ„í—˜ë„: {result.risk_score:.1f}/100 ({result.risk_level})")
        print(f"\nğŸ“Š ë¶„ì„ ê²°ê³¼:")
        print(f"   â€¢ ì •ì±… ìœ„ë°˜: {len(result.violations)}ê±´")
        if result.violations:
            print(f"     â†’ {', '.join(result.violations)}")
        print(f"   â€¢ ìœ„í˜‘ ì§•í›„: {len(result.threats)}ê±´")
        if result.threats:
            for i, threat in enumerate(result.threats[:5], 1):
                print(f"     {i}. {threat}")
        print(f"   â€¢ ì°¸ì¡° ì •ì±…: {', '.join(result.related_policies)}")
        
        # ìœ ì‚¬ë„ ì ìˆ˜ ì¶œë ¥
        if result.policy_similarities:
            print(f"\nğŸ” ì •ì±… ìœ ì‚¬ë„:")
            for policy_id, sim in sorted(
                result.policy_similarities.items(),
                key=lambda x: x[1],
                reverse=True
            ):
                print(f"     {policy_id}: {sim:.3f}")
        
        print(f"\nğŸ’¡ ì„¤ëª…: {result.explanation}")
        print("\n" + "=" * 80 + "\n")
    
    def get_statistics(self) -> Dict:
        """ë¶„ì„ í†µê³„ ë°˜í™˜"""
        cache_info = self._search_policies_cached.cache_info()
        
        return {
            **self.stats,
            'search_mode': self.search_mode,
            'cache_info': {
                'hits': cache_info.hits,
                'misses': cache_info.misses,
                'size': cache_info.currsize,
                'max_size': cache_info.maxsize,
                'hit_rate': (cache_info.hits / max(cache_info.hits + cache_info.misses, 1)) * 100
            }
        }
    
    def print_statistics(self):
        """í†µê³„ ì¶œë ¥"""
        stats = self.get_statistics()
        
        print("\n" + "=" * 80)
        print("ğŸ“Š ë¶„ì„ í†µê³„")
        print("=" * 80)
        print(f"ê²€ìƒ‰ ëª¨ë“œ: {stats['search_mode']}")
        print(f"ì´ ë¶„ì„: {stats['total_analyzed']}ê±´")
        print(f"LLM ì‚¬ìš©: {stats['llm_calls']}ê±´")
        print(f"ê·œì¹™ ê¸°ë°˜: {stats['rule_based_calls']}ê±´")
        print(f"ì˜¤ë¥˜: {stats['errors']}ê±´")
        print(f"í‰ê·  ì •ì±… ìœ ì‚¬ë„: {stats['avg_policy_similarity']:.3f}")
        print(f"\nìºì‹œ ì„±ëŠ¥:")
        print(f"  íˆíŠ¸: {stats['cache_info']['hits']}íšŒ")
        print(f"  ë¯¸ìŠ¤: {stats['cache_info']['misses']}íšŒ")
        print(f"  íˆíŠ¸ìœ¨: {stats['cache_info']['hit_rate']:.1f}%")
        print(f"  í˜„ì¬ í¬ê¸°: {stats['cache_info']['size']}/{stats['cache_info']['max_size']}")
        print("=" * 80 + "\n")
    
    def export_results(
        self,
        results: List[AnalysisResult],
        filepath: str,
        format: str = 'json'
    ):
        """ê²°ê³¼ ë‚´ë³´ë‚´ê¸°"""
        try:
            if format == 'json':
                with open(filepath, 'w', encoding='utf-8') as f:
                    json.dump(
                        [r.dict() for r in results],
                        f,
                        ensure_ascii=False,
                        indent=2
                    )
            elif format == 'csv':
                import csv
                with open(filepath, 'w', encoding='utf-8', newline='') as f:
                    if results:
                        writer = csv.DictWriter(f, fieldnames=results[0].dict().keys())
                        writer.writeheader()
                        for r in results:
                            writer.writerow(r.dict())
            
            logger.info(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {filepath}")
            
        except Exception as e:
            logger.error(f"ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        clear_model_cache()
        self._search_policies_cached.cache_clear()
        logger.info("ğŸ§¹ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
